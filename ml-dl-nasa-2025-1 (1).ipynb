{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12818414,
          "sourceType": "datasetVersion",
          "datasetId": 8105818
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=#023F7C> **Machine Learning and Deep Learning**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<font color=#023F7C>**Hi! PARIS DataBootcamp 2024 üöÄ**</font> <br>\n",
        "\n",
        "\n",
        "<img src = https://www.hi-paris.fr/wp-content/uploads/2020/09/logo-hi-paris-retina.png width = \"300\" height = \"200\" >\n",
        "\n",
        "\n",
        "**Name**: ...           <br>\n",
        "**School**: ...         <br>\n",
        "**Group number**: ...    <br>\n",
        "**Track**: ...           <br>\n",
        "**Teaching Assistants**: Thibault Porssut and Farouk Kadri, Machine Learning Research Engineer @ Hi! PARIS"
      ],
      "metadata": {
        "id": "RgMyA5dmwFJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Important guidelines**\n",
        "\n",
        "\n",
        "The RUL (Remaining Useful Life) refers to the remaining lifespan of an aircraft engine before failure. In the context of the NASA C-MAPSS dataset, this term represents the number of operational cycles an engine can still perform before breaking down. **The goal of this practical session** is to build a Machine Learning model that can p**redict this value using sensor data collected during previous cycles.**\n",
        "\n",
        "To achieve this goal, you are provided with three datasets: train_FD001.txt, test_FD001.txt, and RUL_FD001.txt, originating from the NASA C-MAPSS dataset. <br>\n",
        "These datasets contain sensor measurements and operational settings for multiple aircraft engines recorded over a series of operational cycles. Each engine is run until it fails, and the sensor data capture the degradation process over time. <br>\n",
        "The file RUL_FD001.txt contains the actual Remaining Useful Life (RUL) values for the engines in the test set at the point where the test data end.\n",
        "\n",
        "The variable to predict is RUL, which represents the number of operational cycles remaining before the engine fails."
      ],
      "metadata": {
        "id": "epycO8NUwFJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size='5'><u>How to work on this notebook</u>**</font> <br>\n",
        "The notebook is split in two parts: Machine Learning and Deep Learning.\n",
        "\n",
        "- **Beginner track**: You only have to complete the Machine Learning and Deeep Learning section.\n",
        "- **Intermediate track**: Please complete Machine Learning, Deep Learning and the optional section."
      ],
      "metadata": {
        "id": "yISGPg8WwFJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size='5'><u>Bootcamp deliverables</u>**</font> <br>\n",
        "\n",
        "Here are the two deadlines for the bootcamp deliverables:\n",
        "- <u>**Friday 12:30 PM**</u>: <br> Send us the \"Machine Learning and Deep Learning\" notebooks (no need to send us data cleaning) <br>\n",
        "    - **Each group member should send his own notebooks** (we won't accept one notebook per group)\n",
        "    - Don't forget to complete the start of the notebook with your information (name, school, group number and track)\n",
        "    \n",
        "- <u>**Friday 2:30PM**</u>: <br>Send us the group slides <br>\n",
        "    - You can send us a single powerpoint per group (no need to send us one per group member)\n",
        "    - Don't forget to add your group number as well as who is in your group (name, school and track) to the slides\n",
        "\n",
        "Send both the notebooks and the slides at `data-event@hi-paris.fr`"
      ],
      "metadata": {
        "id": "SGP2HYohwFJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font size='5'><u>Need help ? üôè</u>**</font> <br>\n",
        "\n",
        "We will drop later in the week to the Machine Learning course (Beginner track) on HFactory the `Machine_Learning_Beginner_DB2025.ipynb` notebook for those who need help with the Machine Learning part.\n",
        "\n",
        "**Don't hesitate to ask questions to the bootcamp organizers/staff members if you need help.**\n"
      ],
      "metadata": {
        "id": "1qfwFdDPwFJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Machine Learning**\n",
        "\n",
        "Let's start by importing the libraries needed for this notebook."
      ],
      "metadata": {
        "id": "xCheKJQBwFJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "ZyhKqCkh5D8S",
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:14.519203Z",
          "iopub.execute_input": "2025-08-20T15:12:14.519498Z",
          "iopub.status.idle": "2025-08-20T15:12:14.835280Z",
          "shell.execute_reply.started": "2025-08-20T15:12:14.519477Z",
          "shell.execute_reply": "2025-08-20T15:12:14.834273Z"
        }
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now load the train and test datasets using `pd.read_csv()`"
      ],
      "metadata": {
        "id": "XcOZ9GW8wFJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path =  '02_df_train_FD001_wo_nan_denoised.csv'\n",
        "test_path = '02_df_test_FD001_wo_nan_denoised.csv'\n",
        "\n",
        "# Train and test data.\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)"
      ],
      "metadata": {
        "id": "dVLd2Z_jwFJT",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94b87d3-e437-4250-b435-65c1ece81383",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:15.717065Z",
          "iopub.execute_input": "2025-08-20T15:12:15.717458Z",
          "iopub.status.idle": "2025-08-20T15:12:16.003839Z",
          "shell.execute_reply.started": "2025-08-20T15:12:15.717437Z",
          "shell.execute_reply": "2025-08-20T15:12:16.002698Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (18816, 28)\n",
            "Test shape: (12039, 28)\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Data preprocessing**"
      ],
      "metadata": {
        "id": "Hr9tR1BnwFJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n"
      ],
      "metadata": {
        "id": "jfuHVWn9wFJV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:17.438510Z",
          "iopub.execute_input": "2025-08-20T15:12:17.439085Z",
          "iopub.status.idle": "2025-08-20T15:12:18.116837Z",
          "shell.execute_reply.started": "2025-08-20T15:12:17.439061Z",
          "shell.execute_reply": "2025-08-20T15:12:18.115758Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_with_gaussian_bins(df, cols):\n",
        "    \"\"\"\n",
        "    Replace numerical columns with 7-category Gaussian bins based on mean ¬± n*std.\n",
        "\n",
        "    Parameters:\n",
        "    df   : DataFrame\n",
        "    cols : list of column names to transform\n",
        "\n",
        "    Returns:\n",
        "    DataFrame with the same structure but selected numeric columns replaced with categorical bins\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    labels = [\n",
        "        \"Extremely Low\",\n",
        "        \"Very Low\",\n",
        "        \"Low\",\n",
        "        \"Normal\",\n",
        "        \"High\",\n",
        "        \"Very High\",\n",
        "        \"Extremely High\"\n",
        "    ]\n",
        "\n",
        "    for col in cols:\n",
        "        mu = df_out[col].mean()\n",
        "        sigma = df_out[col].std()\n",
        "\n",
        "        bins = [\n",
        "          -np.inf,\n",
        "          mu - 2*sigma,   # Extremely Low\n",
        "          mu - 1*sigma,   # Very Low\n",
        "          mu - 0.5*sigma,   # Low\n",
        "          mu + 0.5*sigma,   # High\n",
        "          mu + 1*sigma,   # Very High\n",
        "          mu + 2*sigma,   # Extremely High\n",
        "          np.inf\n",
        "      ]\n",
        "\n",
        "        df_out[col] = pd.cut(df_out[col], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "    return df_out"
      ],
      "metadata": {
        "id": "eO2HwJWVoRsb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:19.156858Z",
          "iopub.execute_input": "2025-08-20T15:12:19.157275Z",
          "iopub.status.idle": "2025-08-20T15:12:19.164076Z",
          "shell.execute_reply.started": "2025-08-20T15:12:19.157252Z",
          "shell.execute_reply": "2025-08-20T15:12:19.162994Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=replace_with_gaussian_bins(train_df,[\"HPC outlet temperature (¬∞C)\"])\n",
        "test_df=replace_with_gaussian_bins(test_df,[\"HPC outlet temperature (¬∞C)\"])\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "uWH8j9OpDeYa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:20.512862Z",
          "iopub.execute_input": "2025-08-20T15:12:20.513125Z",
          "iopub.status.idle": "2025-08-20T15:12:20.579744Z",
          "shell.execute_reply.started": "2025-08-20T15:12:20.513107Z",
          "shell.execute_reply": "2025-08-20T15:12:20.578554Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "3c1215d3-3ea7-4763-c533-5e3183ed5a97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   engine_unit_number  time_cycles  setting_1  setting_2  setting_3  \\\n",
              "0                   1            1    -0.0007    -0.0004      100.0   \n",
              "1                   1            2     0.0019    -0.0003      100.0   \n",
              "2                   1            3    -0.0043     0.0003      100.0   \n",
              "3                   1            4     0.0007     0.0000      100.0   \n",
              "4                   1            5    -0.0019    -0.0002      100.0   \n",
              "\n",
              "   Fan inlet temperature (¬∞C)  LPC outlet temperature (¬∞C)  \\\n",
              "0                        15.0                    83.416667   \n",
              "1                        15.0                    83.600000   \n",
              "2                        15.0                    83.711111   \n",
              "3                        15.0                    83.711111   \n",
              "4                        15.0                    83.722222   \n",
              "\n",
              "  HPC outlet temperature (¬∞C)  LPT outlet temperature (¬∞C)  \\\n",
              "0                      Normal                   504.961111   \n",
              "1                      Normal                   506.372222   \n",
              "2                      Normal                   506.961111   \n",
              "3                    Very Low                   505.666667   \n",
              "4                      Normal                   508.083333   \n",
              "\n",
              "   Fan inlet pressure (bar)  ...  Corrected core speed (rpm)  \\\n",
              "0                  1.008014  ...                     8138.62   \n",
              "1                  1.008014  ...                     8131.49   \n",
              "2                  1.008014  ...                     8133.23   \n",
              "3                  1.008014  ...                     8133.83   \n",
              "4                  1.008014  ...                     8133.80   \n",
              "\n",
              "   Bypass ratio (dimensionless)  \\\n",
              "0                        8.4195   \n",
              "1                        8.4318   \n",
              "2                        8.4178   \n",
              "3                        8.3682   \n",
              "4                        8.4294   \n",
              "\n",
              "   Burner fuel-air ratio (mass ratio, dimensionless)  Bleed enthalpy (kJ/kg)  \\\n",
              "0                                               0.03                   392.0   \n",
              "1                                               0.03                   392.0   \n",
              "2                                               0.03                   390.0   \n",
              "3                                               0.03                   392.0   \n",
              "4                                               0.03                   393.0   \n",
              "\n",
              "   Demanded fan speed (rpm)  Demanded corrected fan speed (rpm)  \\\n",
              "0                      2388                               100.0   \n",
              "1                      2388                               100.0   \n",
              "2                      2388                               100.0   \n",
              "3                      2388                               100.0   \n",
              "4                      2388                               100.0   \n",
              "\n",
              "   HPT coolant bleed flow (kg/s)  LPT coolant bleed flow (kg/s)  RUL  \\\n",
              "0                      17.717318                      10.622680  191   \n",
              "1                      17.690102                      10.624766  190   \n",
              "2                      17.667423                      10.588751  189   \n",
              "3                      17.635671                      10.602223  188   \n",
              "4                      17.644743                      10.616057  187   \n",
              "\n",
              "   RUL_class  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  \n",
              "\n",
              "[5 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8cb540f-3f8f-456c-9a02-3fa52231f334\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>engine_unit_number</th>\n",
              "      <th>time_cycles</th>\n",
              "      <th>setting_1</th>\n",
              "      <th>setting_2</th>\n",
              "      <th>setting_3</th>\n",
              "      <th>Fan inlet temperature (¬∞C)</th>\n",
              "      <th>LPC outlet temperature (¬∞C)</th>\n",
              "      <th>HPC outlet temperature (¬∞C)</th>\n",
              "      <th>LPT outlet temperature (¬∞C)</th>\n",
              "      <th>Fan inlet pressure (bar)</th>\n",
              "      <th>...</th>\n",
              "      <th>Corrected core speed (rpm)</th>\n",
              "      <th>Bypass ratio (dimensionless)</th>\n",
              "      <th>Burner fuel-air ratio (mass ratio, dimensionless)</th>\n",
              "      <th>Bleed enthalpy (kJ/kg)</th>\n",
              "      <th>Demanded fan speed (rpm)</th>\n",
              "      <th>Demanded corrected fan speed (rpm)</th>\n",
              "      <th>HPT coolant bleed flow (kg/s)</th>\n",
              "      <th>LPT coolant bleed flow (kg/s)</th>\n",
              "      <th>RUL</th>\n",
              "      <th>RUL_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>-0.0004</td>\n",
              "      <td>100.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>83.416667</td>\n",
              "      <td>Normal</td>\n",
              "      <td>504.961111</td>\n",
              "      <td>1.008014</td>\n",
              "      <td>...</td>\n",
              "      <td>8138.62</td>\n",
              "      <td>8.4195</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392.0</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>17.717318</td>\n",
              "      <td>10.622680</td>\n",
              "      <td>191</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>83.600000</td>\n",
              "      <td>Normal</td>\n",
              "      <td>506.372222</td>\n",
              "      <td>1.008014</td>\n",
              "      <td>...</td>\n",
              "      <td>8131.49</td>\n",
              "      <td>8.4318</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392.0</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>17.690102</td>\n",
              "      <td>10.624766</td>\n",
              "      <td>190</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.0043</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>100.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>83.711111</td>\n",
              "      <td>Normal</td>\n",
              "      <td>506.961111</td>\n",
              "      <td>1.008014</td>\n",
              "      <td>...</td>\n",
              "      <td>8133.23</td>\n",
              "      <td>8.4178</td>\n",
              "      <td>0.03</td>\n",
              "      <td>390.0</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>17.667423</td>\n",
              "      <td>10.588751</td>\n",
              "      <td>189</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>83.711111</td>\n",
              "      <td>Very Low</td>\n",
              "      <td>505.666667</td>\n",
              "      <td>1.008014</td>\n",
              "      <td>...</td>\n",
              "      <td>8133.83</td>\n",
              "      <td>8.3682</td>\n",
              "      <td>0.03</td>\n",
              "      <td>392.0</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>17.635671</td>\n",
              "      <td>10.602223</td>\n",
              "      <td>188</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.0019</td>\n",
              "      <td>-0.0002</td>\n",
              "      <td>100.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>83.722222</td>\n",
              "      <td>Normal</td>\n",
              "      <td>508.083333</td>\n",
              "      <td>1.008014</td>\n",
              "      <td>...</td>\n",
              "      <td>8133.80</td>\n",
              "      <td>8.4294</td>\n",
              "      <td>0.03</td>\n",
              "      <td>393.0</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.0</td>\n",
              "      <td>17.644743</td>\n",
              "      <td>10.616057</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 28 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8cb540f-3f8f-456c-9a02-3fa52231f334')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8cb540f-3f8f-456c-9a02-3fa52231f334 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8cb540f-3f8f-456c-9a02-3fa52231f334');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b6a390dd-f06f-4394-bbf9-90b6cf58416d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6a390dd-f06f-4394-bbf9-90b6cf58416d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b6a390dd-f06f-4394-bbf9-90b6cf58416d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "features = ['engine_unit_number',\n",
        "  'time_cycles',\n",
        "  'LPC outlet temperature (¬∞C)',\n",
        "  \"HPC outlet temperature (¬∞C)\",\n",
        "  \"LPT outlet temperature (¬∞C)\",\n",
        "  \"HPC outlet pressure (bar)\",\n",
        "  \"Physical core speed (rpm)\",\n",
        "  \"Fuel flow / Ps30 (kg/s/bar)\",\n",
        "  \"Corrected fan speed (rpm)\",\n",
        "  \"Bypass ratio (dimensionless)\",\n",
        "  \"Bleed enthalpy (kJ/kg)\",\n",
        "  \"HPT coolant bleed flow (kg/s)\"]\n",
        "\n",
        "\n",
        "\n",
        "X_train = train_df[features].copy()\n",
        "y_train = train_df[\"RUL_class\"].copy()\n",
        "\n",
        "X_test = test_df[features].copy()\n",
        "y_test = test_df[\"RUL_class\"].copy()\n",
        "\n",
        "\n",
        "print(\"Train rows:\", X_train.shape[0], \"Test engines:\", X_test.shape[0])"
      ],
      "metadata": {
        "id": "z1yz5wfVwXi0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:22.140786Z",
          "iopub.execute_input": "2025-08-20T15:12:22.141057Z",
          "iopub.status.idle": "2025-08-20T15:12:22.154475Z",
          "shell.execute_reply.started": "2025-08-20T15:12:22.141040Z",
          "shell.execute_reply": "2025-08-20T15:12:22.153456Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76923688-5c6d-49bb-92d8-1ae9eaf487c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rows: 18816 Test engines: 12039\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Question 2.1.1:</u>** <br>**Transform the categorical variables in each split with `OneHotEncoder`.** <br>\n",
        "\n",
        "\n",
        "The column names OneHotEncoder creates can be accessed with `.get_feature_names_out()`. <br>\n",
        "Go this [page](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for more info on how to use scikit-learn's `OneHotEncoder` function. <br>\n",
        "\n",
        "*Don't forget, data preprocessing is only applied to the feature variables in the case of binary classification !*\n"
      ],
      "metadata": {
        "id": "-4oesPDawFJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = [\"HPC outlet temperature (¬∞C)\"]\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "# Fit the encoder to the training data and transform it.\n",
        "ohe_train_transformed = ohe.fit_transform(X_train[categorical_features])\n",
        "\n",
        "# Get the names for the new columns from the encoder.\n",
        "ohe_feature_names = ohe.get_feature_names_out(categorical_features)\n",
        "\n",
        "# Create a new DataFrame with the encoded features, preserving the original index.\n",
        "X_train_ohe = pd.DataFrame(ohe_train_transformed, columns=ohe_feature_names, index=X_train.index)\n",
        "\n",
        "# Remove the original categorical column and add the new encoded columns.\n",
        "X_train_processed = pd.concat([X_train.drop(columns=categorical_features), X_train_ohe], axis=1)\n",
        "\n",
        "# Transform the test data using the already-fitted encoder.\n",
        "ohe_test_transformed = ohe.transform(X_test[categorical_features])\n",
        "\n",
        "# Create a DataFrame for the test set's new encoded features.\n",
        "X_test_ohe = pd.DataFrame(ohe_test_transformed, columns=ohe_feature_names, index=X_test.index)\n",
        "\n",
        "# Combine the numerical features with the encoded features for the test set.\n",
        "X_test_processed = pd.concat([X_test.drop(columns=categorical_features), X_test_ohe], axis=1)\n",
        "\n",
        "# Display the results.\n",
        "print(\"Processed Training Data Head:\")\n",
        "print(X_train_processed.head())\n",
        "print(\"\\nProcessed Training Data Shape:\")\n",
        "print(X_train_processed.shape)\n",
        "print(\"\\nProcessed Test Data Head:\")\n",
        "print(X_test_processed.head())\n",
        "print(\"\\nProcessed Test Data Shape:\")\n",
        "print(X_test_processed.shape)\n"
      ],
      "metadata": {
        "id": "2YWJ9DzIA3VH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:23.356900Z",
          "iopub.execute_input": "2025-08-20T15:12:23.357603Z",
          "iopub.status.idle": "2025-08-20T15:12:23.399572Z",
          "shell.execute_reply.started": "2025-08-20T15:12:23.357576Z",
          "shell.execute_reply": "2025-08-20T15:12:23.398379Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbee4c3d-4c96-49d3-ee96-f662483feebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Training Data Head:\n",
            "   engine_unit_number  time_cycles  LPC outlet temperature (¬∞C)  \\\n",
            "0                   1            1                    83.416667   \n",
            "1                   1            2                    83.600000   \n",
            "2                   1            3                    83.711111   \n",
            "3                   1            4                    83.711111   \n",
            "4                   1            5                    83.722222   \n",
            "\n",
            "   LPT outlet temperature (¬∞C)  HPC outlet pressure (bar)  \\\n",
            "0                   504.961111                  38.221777   \n",
            "1                   506.372222                  38.179719   \n",
            "2                   506.961111                  38.214882   \n",
            "3                   505.666667                  38.227982   \n",
            "4                   508.083333                  38.196955   \n",
            "\n",
            "   Physical core speed (rpm)  Fuel flow / Ps30 (kg/s/bar)  \\\n",
            "0                    9046.19                    35.967191   \n",
            "1                    9044.07                    36.009938   \n",
            "2                    9052.94                    36.019591   \n",
            "3                    9049.48                    36.049928   \n",
            "4                    9055.15                    36.003733   \n",
            "\n",
            "   Corrected fan speed (rpm)  Bypass ratio (dimensionless)  \\\n",
            "0                    2388.02                        8.4195   \n",
            "1                    2388.07                        8.4318   \n",
            "2                    2388.03                        8.4178   \n",
            "3                    2388.08                        8.3682   \n",
            "4                    2388.04                        8.4294   \n",
            "\n",
            "   Bleed enthalpy (kJ/kg)  HPT coolant bleed flow (kg/s)  \\\n",
            "0                   392.0                      17.717318   \n",
            "1                   392.0                      17.690102   \n",
            "2                   390.0                      17.667423   \n",
            "3                   392.0                      17.635671   \n",
            "4                   393.0                      17.644743   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_Extremely High  \\\n",
            "0                                         0.0   \n",
            "1                                         0.0   \n",
            "2                                         0.0   \n",
            "3                                         0.0   \n",
            "4                                         0.0   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_Extremely Low  \\\n",
            "0                                        0.0   \n",
            "1                                        0.0   \n",
            "2                                        0.0   \n",
            "3                                        0.0   \n",
            "4                                        0.0   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_High  HPC outlet temperature (¬∞C)_Low  \\\n",
            "0                               0.0                              0.0   \n",
            "1                               0.0                              0.0   \n",
            "2                               0.0                              0.0   \n",
            "3                               0.0                              0.0   \n",
            "4                               0.0                              0.0   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_Normal  HPC outlet temperature (¬∞C)_Very High  \\\n",
            "0                                 1.0                                    0.0   \n",
            "1                                 1.0                                    0.0   \n",
            "2                                 1.0                                    0.0   \n",
            "3                                 0.0                                    0.0   \n",
            "4                                 1.0                                    0.0   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_Very Low  \n",
            "0                                   0.0  \n",
            "1                                   0.0  \n",
            "2                                   0.0  \n",
            "3                                   1.0  \n",
            "4                                   0.0  \n",
            "\n",
            "Processed Training Data Shape:\n",
            "(18816, 18)\n",
            "\n",
            "Processed Test Data Head:\n",
            "   engine_unit_number  time_cycles  LPC outlet temperature (¬∞C)  \\\n",
            "0                   1            1                    84.083333   \n",
            "1                   1            2                    83.355556   \n",
            "2                   1            3                    83.772222   \n",
            "3                   1            4                    83.761111   \n",
            "4                   1            5                    83.800000   \n",
            "\n",
            "   LPT outlet temperature (¬∞C)  HPC outlet pressure (bar)  \\\n",
            "0                   503.633333                  38.190061   \n",
            "1                   502.083333                  38.255561   \n",
            "2                   505.372222                  38.204540   \n",
            "3                   508.194444                  38.201782   \n",
            "4                   505.694444                  38.207987   \n",
            "\n",
            "   Physical core speed (rpm)  Fuel flow / Ps30 (kg/s/bar)  \\\n",
            "0                    9050.17                    35.971328   \n",
            "1                    9054.42                    36.001665   \n",
            "2                    9056.96                    35.988565   \n",
            "3                    9045.29                    35.947886   \n",
            "4                    9044.55                    36.000975   \n",
            "\n",
            "   Corrected fan speed (rpm)  Bypass ratio (dimensionless)  \\\n",
            "0                    2388.03                        8.4052   \n",
            "1                    2388.06                        8.3803   \n",
            "2                    2388.03                        8.4441   \n",
            "3                    2388.05                        8.3917   \n",
            "4                    2388.03                        8.4031   \n",
            "\n",
            "   Bleed enthalpy (kJ/kg)  HPT coolant bleed flow (kg/s)  \\\n",
            "0                     392                      17.626599   \n",
            "1                     393                      17.699174   \n",
            "2                     393                      17.726390   \n",
            "3                     391                      17.690102   \n",
            "4                     390                      17.685567   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_Extremely High  \\\n",
            "0                                         0.0   \n",
            "1                                         0.0   \n",
            "2                                         0.0   \n",
            "3                                         0.0   \n",
            "4                                         0.0   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_Extremely Low  \\\n",
            "0                                        0.0   \n",
            "1                                        0.0   \n",
            "2                                        0.0   \n",
            "3                                        0.0   \n",
            "4                                        0.0   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_High  HPC outlet temperature (¬∞C)_Low  \\\n",
            "0                               0.0                              1.0   \n",
            "1                               0.0                              0.0   \n",
            "2                               0.0                              0.0   \n",
            "3                               0.0                              1.0   \n",
            "4                               0.0                              0.0   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_Normal  HPC outlet temperature (¬∞C)_Very High  \\\n",
            "0                                 0.0                                    0.0   \n",
            "1                                 1.0                                    0.0   \n",
            "2                                 1.0                                    0.0   \n",
            "3                                 0.0                                    0.0   \n",
            "4                                 1.0                                    0.0   \n",
            "\n",
            "   HPC outlet temperature (¬∞C)_Very Low  \n",
            "0                                   0.0  \n",
            "1                                   0.0  \n",
            "2                                   0.0  \n",
            "3                                   0.0  \n",
            "4                                   0.0  \n",
            "\n",
            "Processed Test Data Shape:\n",
            "(12039, 18)\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "However label encoding is preferred over one-hot encoding for ordinal labels because ordinal features have a meaningful, intrinsic order (e.g., Low < Medium < High). Label encoding preserves this natural ranking by mapping categories to integers that reflect their order, allowing models to interpret the progression between categories. In contrast, one-hot encoding would treat each category as unrelated, discarding the ordinal relationship and increasing the number of features unnecessarily, which can reduce efficiency without adding value."
      ],
      "metadata": {
        "id": "AGCZAcSPW3Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Question 2.1.2:</u>** <br>**Transform the categorical variables with `LabelEncoder`.** <br>\n",
        "\n",
        "\n",
        "The column names LabelEncoder creates can be accessed with `.get_feature_names_out()`. <br>\n",
        "Go this [page](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) for more info on how to use scikit-learn's `LabelEncoder` function. <br>\n",
        "\n",
        "*Don't forget, data preprocessing is only applied to the feature variables in the case of binary classification !*"
      ],
      "metadata": {
        "id": "1glnMTcVOoZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = X_train.select_dtypes(include=['category']).columns\n",
        "print(\"Categorical columns:\", categorical_cols)\n",
        "\n",
        "# Initialize the LabelEncoder.\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the training data and transform it.\n",
        "X_train_le = le.fit_transform(X_train[categorical_cols].values.ravel())\n",
        "# Transform the test data using the already-fitted encoder.\n",
        "X_test_le = le.transform(X_test[categorical_cols].values.ravel())\n",
        "\n",
        "X_train_le = pd.DataFrame(X_train_le, columns=categorical_cols)\n",
        "X_test_le = pd.DataFrame(X_test_le, columns=categorical_cols)\n",
        "\n",
        "# Display the results to show the transformation.\n",
        "print(\"Processed Training Data with LabelEncoder (Head):\")\n",
        "print(X_train_le.head())\n",
        "print(\"\\nShape of Processed Training Data:\")\n",
        "print(X_train_le.shape)\n",
        "\n",
        "print(\"\\nProcessed Test Data with LabelEncoder (Head):\")\n",
        "print(X_test_le.head())\n",
        "print(\"\\nShape of Processed Test Data:\")\n",
        "print(X_test_le.shape)"
      ],
      "metadata": {
        "id": "W51mOKEw3Ery",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:27.357166Z",
          "iopub.execute_input": "2025-08-20T15:12:27.357478Z",
          "iopub.status.idle": "2025-08-20T15:12:27.381033Z",
          "shell.execute_reply.started": "2025-08-20T15:12:27.357454Z",
          "shell.execute_reply": "2025-08-20T15:12:27.380147Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca3b2b7-613b-4907-d96f-681a2ebc76b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns: Index(['HPC outlet temperature (¬∞C)'], dtype='object')\n",
            "Processed Training Data with LabelEncoder (Head):\n",
            "   HPC outlet temperature (¬∞C)\n",
            "0                            4\n",
            "1                            4\n",
            "2                            4\n",
            "3                            6\n",
            "4                            4\n",
            "\n",
            "Shape of Processed Training Data:\n",
            "(18816, 1)\n",
            "\n",
            "Processed Test Data with LabelEncoder (Head):\n",
            "   HPC outlet temperature (¬∞C)\n",
            "0                            3\n",
            "1                            4\n",
            "2                            4\n",
            "3                            3\n",
            "4                            4\n",
            "\n",
            "Shape of Processed Test Data:\n",
            "(12039, 1)\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the mapping between original categories and encoded numerical values\n",
        "print(\"Mapping of categories to numerical values:\")\n",
        "for i, category in enumerate(le.classes_):\n",
        "    print(f\"{category}: {i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpT9Sl9WP-Hb",
        "outputId": "6c3cad38-b2ee-444b-da8a-788bc25788b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping of categories to numerical values:\n",
            "Extremely High: 0\n",
            "Extremely Low: 1\n",
            "High: 2\n",
            "Low: 3\n",
            "Normal: 4\n",
            "Very High: 5\n",
            "Very Low: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The order obtained doesn't follow strictly the natural order. We'll go for a more natural way to encode this categorical feature"
      ],
      "metadata": {
        "id": "2sbpfPQzrEGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use a manual mapping first,because the category is ordinal(or we can just use OrdinalEncoder to begin with)\n",
        "# The default encoding for LabelEncoder uses alphabetical sorting which would give close encoding both to \"Extremeley Low\" and \"Extremely High\"\n",
        "# Now to use manual encoding that takes into account temperature order\n",
        "# Create copies of the original dataframes to work with.\n",
        "X_train_manual = X_train.copy()\n",
        "X_test_manual = X_test.copy()\n",
        "\n",
        "# Define the explicit order of the categories from lowest to highest.\n",
        "temperature_mapping = {\n",
        "    \"Extremely Low\": 0,\n",
        "    \"Very Low\": 1,\n",
        "    \"Low\": 2,\n",
        "    \"Normal\": 3,\n",
        "    \"High\": 4,\n",
        "    \"Very High\": 5,\n",
        "    \"Extremely High\": 6\n",
        "}\n",
        "\n",
        "# The categorical column to be transformed.\n",
        "categorical_feature = \"HPC outlet temperature (¬∞C)\"\n",
        "\n",
        "# Apply the custom mapping to the column in both the training and test sets.\n",
        "X_train_manual[categorical_feature] = X_train_manual[categorical_feature].map(temperature_mapping)\n",
        "X_test_manual[categorical_feature] = X_test_manual[categorical_feature].map(temperature_mapping)\n",
        "\n",
        "\n",
        "# Display the results to verify the manual encoding.\n",
        "print(\"Processed Training Data with Manual Encoding (Head):\")\n",
        "print(X_train_manual.head())\n",
        "print(\"\\nShape of Processed Training Data:\")\n",
        "print(X_train_manual.shape)\n",
        "\n",
        "print(\"\\nProcessed Test Data with Manual Encoding (Head):\")\n",
        "print(X_test_manual.head())\n",
        "print(\"\\nShape of Processed Test Data:\")\n",
        "print(X_test_manual.shape)"
      ],
      "metadata": {
        "id": "oW51vl2rq4_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remark: The mapping is not following perfectly the natural order. We'll go for something else"
      ],
      "metadata": {
        "id": "jEZKH5E5Qdgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<u>Question 2.1.3</u>**: <br>\n",
        "**Scale the continuous variables using either `StandardScaler` (standardization) or `MinMaxScaler` (normalization).**<br>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UwttVkxewFJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HPC is exempted from scaling because we're counting it as categorical and not continuous in our case\n",
        "# Use the categorical_cols defined earlier to exclude them from continuous features\n",
        "continuous_features = [col for col in X_train_manual.columns if col not in categorical_cols]\n",
        "\n",
        "# --- Method 1: StandardScaler ---\n",
        "# Create a copy to store the standardized data.\n",
        "X_train_scaled = X_train_manual.copy()\n",
        "X_test_scaled = X_test_manual.copy()\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on the training data and transform both train and test data.\n",
        "X_train_scaled[continuous_features] = scaler.fit_transform(X_train_scaled[continuous_features])\n",
        "X_test_scaled[continuous_features] = scaler.transform(X_test_scaled[continuous_features])\n",
        "\n",
        "print(\"--- Data after StandardScaler ---\")\n",
        "print(\"Processed Training Data Head (Standardized):\")\n",
        "print(X_train_scaled.head())"
      ],
      "metadata": {
        "id": "7Dpy9DLf3Ery",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:32.036401Z",
          "iopub.execute_input": "2025-08-20T15:12:32.036734Z",
          "iopub.status.idle": "2025-08-20T15:12:32.106560Z",
          "shell.execute_reply.started": "2025-08-20T15:12:32.036707Z",
          "shell.execute_reply": "2025-08-20T15:12:32.105600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "24d6c51e-701e-44bb-a661-eeeefc3e2268"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_manual' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-854377507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# HPC is exempted from scaling because we're counting it as categorical and not continuous in our case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Use the categorical_cols defined earlier to exclude them from continuous features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcontinuous_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_manual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# --- Method 1: StandardScaler ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_manual' is not defined"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Method 2: MinMaxScaler ---\n",
        "# Create a copy to store the normalized data.\n",
        "X_train_normalized = X_train_manual.copy()\n",
        "X_test_normalized = X_test_manual.copy()\n",
        "\n",
        "# Initialize the MinMaxScaler.\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# Fit on the training data and transform both train and test data.\n",
        "X_train_normalized[continuous_features] = min_max_scaler.fit_transform(X_train_normalized[continuous_features])\n",
        "X_test_normalized[continuous_features] = min_max_scaler.transform(X_test_normalized[continuous_features])\n",
        "\n",
        "X_train_scaled = X_train_normalized\n",
        "X_test_scaled = X_test_normalized\n",
        "\n",
        "print(\"\\n\\n--- Data after MinMaxScaler ---\")\n",
        "print(\"Processed Training Data Head (Normalized):\")\n",
        "print(X_train_normalized.head())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:33.667881Z",
          "iopub.execute_input": "2025-08-20T15:12:33.668173Z",
          "iopub.status.idle": "2025-08-20T15:12:33.691717Z",
          "shell.execute_reply.started": "2025-08-20T15:12:33.668153Z",
          "shell.execute_reply": "2025-08-20T15:12:33.690850Z"
        },
        "id": "yeOr8cBxxd1Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Model training and evaluation**\n",
        "Now that our dataset has been preprocessed, we can use it to train Machine Learning models.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K9P9cvpH73he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics for evaluation\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
        "    classification_report, roc_auc_score, PrecisionRecallDisplay, average_precision_score\n",
        ")\n",
        "\n",
        "# hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Classification algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Plotting & Style\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n"
      ],
      "metadata": {
        "id": "lrOnwmzxLGdL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:35.578507Z",
          "iopub.execute_input": "2025-08-20T15:12:35.578847Z",
          "iopub.status.idle": "2025-08-20T15:12:36.442106Z",
          "shell.execute_reply.started": "2025-08-20T15:12:35.578827Z",
          "shell.execute_reply": "2025-08-20T15:12:36.441194Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train three models of your choice** (Logistic Regression, K nearest neighbor, Decision Tree,...) **using scikit-learn's `.fit()` method. <br>**\n",
        "\n",
        "<u>Help</u>: Train these models on the training set (`X_train` and `y_train`).\n",
        "\n",
        "advice: Try Random Forest\n",
        "\n"
      ],
      "metadata": {
        "id": "XNL0EcTFwFJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why classification instead of regression?**\n",
        "\n",
        "Although the Remaining Useful Life (RUL) is a continuous variable, here we approach the problem as classification because:\n",
        "\n",
        "- Classification simplifies decision-making by focusing on whether an engine is likely to fail soon (within 30 cycles) rather than predicting the exact remaining cycles.\n",
        "- Regression models on RUL often struggle to be robust due to noise and variability in the sensor data.\n",
        "- Classification allows for more stable, actionable predictions aligned with maintenance needs: \"replace soon\" vs \"safe\"."
      ],
      "metadata": {
        "id": "NOqEmiJV7x-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.1**:</u> <br>  Choose any three models to try. Which ones do you pick ?"
      ],
      "metadata": {
        "id": "QkHDWOJsjviz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Gradient Boosting Classifier\n",
        "2. K-NN\n",
        "3. Random Forest"
      ],
      "metadata": {
        "id": "nux2T3aX3Ery"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.2**:</u> <br> Train the FIRST selected model using .fit(X_train_scaled, y_train)."
      ],
      "metadata": {
        "id": "J6LvCTBQj3uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Logistic Regression ---\n",
        "\n",
        "print(\"--- 1. Training Gradient Boosting Classifier Model ---\")\n",
        "\n",
        "# Initialize the model\n",
        "grad_boost = GradientBoostingClassifier()\n",
        "grad_boost.fit(X_train_scaled, y_train)\n",
        "print(\"Gradient Boosting Classifier Model trained successfully.\")"
      ],
      "metadata": {
        "id": "o_RDVxx53Ery",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:39.795384Z",
          "iopub.execute_input": "2025-08-20T15:12:39.796145Z",
          "iopub.status.idle": "2025-08-20T15:12:39.977978Z",
          "shell.execute_reply.started": "2025-08-20T15:12:39.796118Z",
          "shell.execute_reply": "2025-08-20T15:12:39.976645Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.3**:</u> <br> Train the SECOND and THIRD selected models."
      ],
      "metadata": {
        "id": "GuAK_ClYkWfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Train K-Nearest Neighbors (KNN) Model ---\n",
        "print(\"\\nTraining the K-Nearest Neighbors (KNN) model...\")\n",
        "# Initialize the model (using default n_neighbors=5).\n",
        "knn = KNeighborsClassifier()\n",
        "# Train the model on the scaled training data.\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "print(\"KNN model trained successfully.\")\n",
        "# --- 3. Train Random Forest Model ---\n",
        "print(\"\\nTraining the Random Forest model...\")\n",
        "# Initialize the model with a random_state for reproducibility.\n",
        "random_forest = RandomForestClassifier(random_state=42)\n",
        "# Train the model on the scaled training data.\n",
        "random_forest.fit(X_train_scaled, y_train)\n",
        "print(\"Random Forest model trained successfully.\")"
      ],
      "metadata": {
        "id": "EP4racwN3Erz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:41.645123Z",
          "iopub.execute_input": "2025-08-20T15:12:41.645772Z",
          "iopub.status.idle": "2025-08-20T15:12:44.553235Z",
          "shell.execute_reply.started": "2025-08-20T15:12:41.645741Z",
          "shell.execute_reply": "2025-08-20T15:12:44.552091Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.4**:</u> <br> FIRST model: predict on TRAIN/TEST, then print Test Accuracy, Test Recall, Test F1.\n",
        "If our goal is to catch failures, which metric should we prioritize?\n",
        "\n",
        "*Help: The .predict() function should be used on the feature of the test set (X_test)*. [page](https://scikit-learn.org/stable/getting_started.html)\n",
        "\n",
        "accuracy_score(), recall_score(), f1_score()\n",
        "find all metrics here: [page](https://scikit-learn.org/stable/api/sklearn.metrics.html)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NKYHRbb0ZXWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Predict on TRAIN and TEST sets using the Gradient Boosting Classifier Model ---\n",
        "print(\"Generating predictions with the Gradient Boosting Classifier Model...\")\n",
        "y_train_pred_grad_boost = grad_boost.predict(X_train_scaled)\n",
        "y_test_pred_grad_boost = grad_boost.predict(X_test_scaled)\n",
        "print(\"Predictions generated.\")\n",
        "\n",
        "# --- 2. Calculate and Print Test Metrics ---\n",
        "# Calculate the metrics for the TEST set.\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred_grad_boost)\n",
        "test_recall = recall_score(y_test, y_test_pred_grad_boost)\n",
        "test_f1 = f1_score(y_test, y_test_pred_grad_boost)\n",
        "\n",
        "print(\"\\n--- Gradient Boosting Classifier Model - Test Set Performance ---\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Recall:   {test_recall:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f}\")"
      ],
      "metadata": {
        "id": "JuA_hW813Erz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:46.224039Z",
          "iopub.execute_input": "2025-08-20T15:12:46.224309Z",
          "iopub.status.idle": "2025-08-20T15:12:46.250070Z",
          "shell.execute_reply.started": "2025-08-20T15:12:46.224283Z",
          "shell.execute_reply": "2025-08-20T15:12:46.249187Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.5**:</u> <br> Repeat for the SECOND model."
      ],
      "metadata": {
        "id": "XfJNZewTZR93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Predict on TRAIN and TEST sets using the K-NN model ---\n",
        "print(\"Generating predictions with the K-NN model...\")\n",
        "y_train_pred_knn = knn.predict(X_train_scaled)\n",
        "y_test_pred_knn = knn.predict(X_test_scaled)\n",
        "print(\"Predictions generated.\")\n",
        "\n",
        "# --- 2. Calculate and Print Test Metrics ---\n",
        "# Calculate the metrics for the TEST set.\n",
        "test_accuracy_knn = accuracy_score(y_test, y_test_pred_knn)\n",
        "test_recall_knn = recall_score(y_test, y_test_pred_knn)\n",
        "test_f1_knn = f1_score(y_test, y_test_pred_knn)\n",
        "\n",
        "print(\"\\n--- K-NN - Test Set Performance ---\")\n",
        "print(f\"Test Accuracy: {test_accuracy_knn:.4f}\")\n",
        "print(f\"Test Recall:   {test_recall_knn:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1_knn:.4f}\")"
      ],
      "metadata": {
        "id": "B3VOM8lK3Erz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:48.174078Z",
          "iopub.execute_input": "2025-08-20T15:12:48.174734Z",
          "iopub.status.idle": "2025-08-20T15:12:55.466332Z",
          "shell.execute_reply.started": "2025-08-20T15:12:48.174688Z",
          "shell.execute_reply": "2025-08-20T15:12:55.465441Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.6**:</u> <br> Repeat for the THIRD model."
      ],
      "metadata": {
        "id": "MNr4ou0dZMMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Predict on TRAIN and TEST sets using the Random Forest model ---\n",
        "print(\"Generating predictions with the Random Forest model...\")\n",
        "y_train_pred_rf = random_forest.predict(X_train_scaled)\n",
        "y_test_pred_rf = random_forest.predict(X_test_scaled)\n",
        "print(\"Predictions generated.\")\n",
        "\n",
        "# --- 2. Calculate and Print Test Metrics ---\n",
        "# Calculate the metrics for the TEST set.\n",
        "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
        "test_recall_rf = recall_score(y_test, y_test_pred_rf)\n",
        "test_f1_rf = f1_score(y_test, y_test_pred_rf)\n",
        "\n",
        "print(\"\\n--- Random Forest - Test Set Performance ---\")\n",
        "print(f\"Test Accuracy: {test_accuracy_rf:.4f}\")\n",
        "print(f\"Test Recall:   {test_recall_rf:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1_rf:.4f}\")"
      ],
      "metadata": {
        "id": "y5QhDE6a3Erz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:12:59.322155Z",
          "iopub.execute_input": "2025-08-20T15:12:59.322474Z",
          "iopub.status.idle": "2025-08-20T15:12:59.648561Z",
          "shell.execute_reply.started": "2025-08-20T15:12:59.322442Z",
          "shell.execute_reply": "2025-08-20T15:12:59.647585Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice that we emphasize Recall. Recall answers: ‚Äúof all the true failures, how many did the model catch?‚Äù On imbalanced datasets, a model can show high accuracy while still missing many failures (predicting the majority class most of the time). That means lots of false negatives. When the goal is failure detection, Recall is the right metric because it prioritizes catching positives (fewer missed failures), even if that sometimes increases false alarms. Keep in mind there‚Äôs a trade-off: higher Recall can lower Precision, so we pick the threshold (or settings) that gives the best compromise for our use case.\n",
        "\n",
        "- If Accuracy is high but Recall is low, we are missing failures (false negatives).\n",
        "- Since the goal is to detect failures, prioritize Recall."
      ],
      "metadata": {
        "id": "san3ALWru88p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.7**:</u> <br> Find the best model by TEST Recall among the three and show its TEST confusion matrix.\n",
        "\n",
        "*help: Look at recall_score() and confusion_matrix() [page](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)*"
      ],
      "metadata": {
        "id": "dK3BK80sYlw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best model by the highest recall score\n",
        "models_performance = {\n",
        "    \"Gradient Boosting Classifier\": test_recall,\n",
        "    \"K-Nearest Neighbors\": test_recall_knn,\n",
        "    \"Random Forest\": test_recall_rf\n",
        "}\n",
        "best_model_name=max(models_performance,key=models_performance.get)\n",
        "best_model_recall = models_performance[best_model_name]\n",
        "print(\"--- Model Comparison by Test Recall ---\")\n",
        "for model, recall in models_performance.items():\n",
        "    print(f\"{model}: {recall:.4f}\")\n",
        "\n",
        "print(f\"\\nBest Model by Test Recall: {best_model_name} (Recall: {best_model_recall:.4f})\")\n",
        "# Show the Confusion Matrix for the Best Model\n",
        "if best_model_name == \"Random Forest\":\n",
        "    best_model_predictions = y_test_pred_rf\n",
        "elif best_model_name == \"K-Nearest Neighbors\":\n",
        "    best_model_predictions = y_test_pred_knn\n",
        "else:\n",
        "    best_model_predictions = y_test_pred_grad_boost\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, best_model_predictions)\n",
        "\n",
        "# Plot the confusion matrix for better visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted Non-Failure', 'Predicted Failure'],\n",
        "            yticklabels=['Actual Non-Failure', 'Actual Failure'])\n",
        "plt.title(f'Confusion Matrix for {best_model_name}')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DOfM9cp13Er5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:13:24.157803Z",
          "iopub.execute_input": "2025-08-20T15:13:24.158082Z",
          "iopub.status.idle": "2025-08-20T15:13:24.514796Z",
          "shell.execute_reply.started": "2025-08-20T15:13:24.158062Z",
          "shell.execute_reply": "2025-08-20T15:13:24.513937Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.8**:</u> <br> Plot ROC and Precision‚ÄìRecall curves for the BEST model (needs probabilities).\n",
        "*help: Look at roc_curve() [page](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) and precision_recall_curve() [page](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)*"
      ],
      "metadata": {
        "id": "3HOmM2SuYbzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Predicted Probabilities for the Positive Class\n",
        "# The second column [:, 1] corresponds to the probability of the positive class (Failure=1)\n",
        "if best_model_name == \"Random Forest\":\n",
        "    y_probs_rf = random_forest.predict_proba(X_test_scaled)[:, 1]\n",
        "elif best_model_name == \"K-Nearest Neighbors\":\n",
        "    y_probs_rf = knn.predict_proba(X_test_scaled)[:, 1]\n",
        "else:\n",
        "    y_probs_rf = grad_boost.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "#  Calculate Metrics for ROC Curve\n",
        "fpr, tpr, roc_thresholds = roc_curve(y_test, y_probs_rf)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#  Calculate Metrics for Precision-Recall Curve\n",
        "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_probs_rf)\n",
        "avg_precision = average_precision_score(y_test, y_probs_rf)\n",
        "\n",
        "#  Plot the Curves\n",
        "# Create a figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot the ROC Curve\n",
        "ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='No-Skill Classifier')\n",
        "ax1.set_xlim([0.0, 1.0])\n",
        "ax1.set_ylim([0.0, 1.05])\n",
        "ax1.set_xlabel('False Positive Rate')\n",
        "ax1.set_ylabel('True Positive Rate (Recall)')\n",
        "ax1.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
        "ax1.legend(loc=\"lower right\")\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "ax2.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.2f})')\n",
        "ax2.set_xlabel('Recall')\n",
        "ax2.set_ylabel('Precision')\n",
        "ax2.set_ylim([0.0, 1.05])\n",
        "ax2.set_xlim([0.0, 1.0])\n",
        "ax2.set_title('Precision-Recall Curve')\n",
        "ax2.legend(loc=\"lower left\")\n",
        "ax2.grid(True)\n",
        "\n",
        "# Display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zEgdJmHm3Er5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:15:08.630371Z",
          "iopub.execute_input": "2025-08-20T15:15:08.630792Z",
          "iopub.status.idle": "2025-08-20T15:15:12.046800Z",
          "shell.execute_reply.started": "2025-08-20T15:15:08.630767Z",
          "shell.execute_reply": "2025-08-20T15:15:12.045930Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.9**:</u> <br> Pick the threshold with the highest Recall. ONLY on the best model (picked above)"
      ],
      "metadata": {
        "id": "tPVN_ZV6YJv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all indices where recall is maximal\n",
        "# precision, recall, pr_thresholds were calculated in the previous cell (zEgdJmHm3Er5)\n",
        "max_recall_value = np.max(recall)\n",
        "max_recall_indices = np.where(recall == max_recall_value)[0]\n",
        "\n",
        "# Among those, find the index with the highest precision\n",
        "# Note: pr_thresholds has one less element than precision and recall,\n",
        "# so we need to use the index relative to precision/recall and then get the threshold.\n",
        "# If max_recall_indices[best_index] is the last index, the corresponding threshold is the last one in pr_thresholds.\n",
        "if max_recall_indices[-1] == len(pr_thresholds):\n",
        "    best_index = max_recall_indices[np.argmax(precision[max_recall_indices])]\n",
        "    best_threshold = pr_thresholds[best_index -1] # Use the last threshold\n",
        "else:\n",
        "    best_index = max_recall_indices[np.argmax(precision[max_recall_indices])]\n",
        "    best_threshold = pr_thresholds[best_index]\n",
        "\n",
        "\n",
        "# Get the corresponding recall and precision\n",
        "best_recall = recall[best_index]\n",
        "best_precision = precision[best_index]\n",
        "\n",
        "\n",
        "# Step 4: Display results\n",
        "print(f\"--- Optimal Threshold for Best Model (Based on Max Recall and Max Precision among those) ---\")\n",
        "print(f\"Best threshold: {best_threshold:.4f}\")\n",
        "print(f\"Recall at this threshold: {best_recall:.4f}\")\n",
        "print(f\"Precision at this threshold: {best_precision:.4f}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"This threshold ({best_threshold:.4f}) was chosen because it achieves the highest possible recall ({best_recall:.4f}),\")\n",
        "print(f\"and among all thresholds that achieve this maximum recall, it provides the best precision ({best_precision:.4f}).\")\n",
        "print(f\"Using this threshold means you would classify an engine as 'Failure' if its predicted probability of failure is {best_threshold:.4f} or higher.\")"
      ],
      "metadata": {
        "id": "LBp14o4Y3Er5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:18:35.992865Z",
          "iopub.execute_input": "2025-08-20T15:18:35.993126Z",
          "iopub.status.idle": "2025-08-20T15:18:36.001140Z",
          "shell.execute_reply.started": "2025-08-20T15:18:35.993109Z",
          "shell.execute_reply": "2025-08-20T15:18:36.000150Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 2.2.10**:</u> <br> Try GridSearch to favor Recall.\n",
        "\n",
        "*Help: Try GridSearchCV [page](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) on the best model you selected (by Recall) to find hyperparameters that improve Recall.<br>\n",
        "Report the best hyperparameters (.best_params_) and the best cross-validated Recall (.best_score_). Then re-fit the tuned model and print the Test Recall.*\n",
        "\n",
        "Notes (keep it simple):\n",
        "Use scoring='recall' in GridSearchCV (we care most about catching failures).\n",
        "Keep the grid tiny (2‚Äì3 values per parameter) to avoid long runs; set cv=3.\n",
        "Compare against your baseline Test Recall (before tuning). One line is enough.\n",
        "\n",
        "*(Optional) If you tried different decision thresholds earlier (e.g., 0.4 / 0.5 / 0.6), also report Test Recall using your chosen threshold with the tuned model.*"
      ],
      "metadata": {
        "id": "8pkz20L9m-Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Case of K-NN\n",
        "\n",
        "# Define a Simple Hyperparameter Grid for KNN\n",
        "# We'll keep the grid small as requested to ensure it runs quickly.\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['minkowski', 'manhattan']\n",
        "}\n",
        "\n",
        "# Set up and Run GridSearchCV\n",
        "# Initialize the base KNN model\n",
        "knn_base = KNeighborsClassifier()\n",
        "\n",
        "# Set up GridSearchCV to find the best model by maximizing recall\n",
        "grid_search = GridSearchCV(estimator=knn_base,\n",
        "                           param_grid=param_grid,\n",
        "                           scoring='recall',\n",
        "                           cv=3,\n",
        "                           n_jobs=-1)\n",
        "\n",
        "print(\"Running GridSearchCV to find the best hyperparameters for Recall...\")\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "print(\"GridSearchCV finished.\")\n",
        "\n",
        "# Report the Best Hyperparameters and Scores\n",
        "print(\"\\n--- GridSearchCV Results ---\")\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validated Recall: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Re-fit the Tuned Model and Evaluate on Test Set\n",
        "best_knn_tuned = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set with the tuned model\n",
        "y_test_pred_tuned = best_knn_tuned.predict(X_test_scaled)\n",
        "tuned_test_recall = recall_score(y_test, y_test_pred_tuned)\n",
        "\n",
        "# Compare with Baseline\n",
        "baseline_test_recall = test_recall_knn\n",
        "\n",
        "print(\"\\n--- Performance on Test Set ---\")\n",
        "print(f\"Baseline Test Recall (Untuned KNN): {baseline_test_recall:.4f}\")\n",
        "print(f\"Tuned Test Recall (GridSearch KNN): {tuned_test_recall:.4f}\")"
      ],
      "metadata": {
        "id": "TAMrPqRy3Er5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:21:44.979181Z",
          "iopub.execute_input": "2025-08-20T15:21:44.979488Z",
          "iopub.status.idle": "2025-08-20T15:22:20.204291Z",
          "shell.execute_reply.started": "2025-08-20T15:21:44.979467Z",
          "shell.execute_reply": "2025-08-20T15:22:20.203126Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Case of Random Forest\n",
        "\n",
        "# We will try to tune the Random Forest to see if we can get it to outperform the K-NN\n",
        "# Define a Grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "# Run GridSearchCV on Random Forest\n",
        "rf_base = RandomForestClassifier(random_state=42)\n",
        "rf_grid_search = GridSearchCV(estimator=rf_base,\n",
        "                              param_grid=rf_param_grid,\n",
        "                              scoring='recall',\n",
        "                              cv=3,\n",
        "                              n_jobs=-1)\n",
        "\n",
        "print(\"Running GridSearchCV on Random Forest...\")\n",
        "rf_grid_search.fit(X_train_scaled, y_train)\n",
        "print(\"GridSearchCV finished.\")\n",
        "\n",
        "# Report Results\n",
        "print(f\"\\nBest RF Hyperparameters: {rf_grid_search.best_params_}\")\n",
        "best_rf_tuned = rf_grid_search.best_estimator_\n",
        "y_test_pred_rf_tuned = best_rf_tuned.predict(X_test_scaled)\n",
        "tuned_rf_test_recall = recall_score(y_test, y_test_pred_rf_tuned)\n",
        "\n",
        "print(f\"\\nBaseline Random Forest Test Recall: {test_recall_rf:.4f}\")\n",
        "print(f\"Tuned Random Forest Test Recall: {tuned_rf_test_recall:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:26:16.129069Z",
          "iopub.execute_input": "2025-08-20T15:26:16.129335Z",
          "iopub.status.idle": "2025-08-20T15:27:03.000983Z",
          "shell.execute_reply.started": "2025-08-20T15:26:16.129317Z",
          "shell.execute_reply": "2025-08-20T15:27:03.000019Z"
        },
        "id": "W6eEkkOXxd1p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Case of Gradient Boosting Classifier\n",
        "\n",
        "# Define a Simple Hyperparameter Grid for Gradient Boosting Classifier\n",
        "# We'll keep the grid small as requested.\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'max_depth': [3, 5],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Set up and Run GridSearchCV\n",
        "# Initialize the base Gradient Boosting Classifier model\n",
        "gb_base = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV to find the best model by maximizing recall\n",
        "grid_search_gb = GridSearchCV(estimator=gb_base,\n",
        "                              param_grid=gb_param_grid,\n",
        "                              scoring='recall',\n",
        "                              cv=3,\n",
        "                              n_jobs=-1,\n",
        "                              verbose=1) # Verbose shows progress\n",
        "\n",
        "print(\"Running GridSearchCV on Gradient Boosting Classifier model...\")\n",
        "grid_search_gb.fit(X_train_scaled, y_train)\n",
        "print(\"GridSearchCV finished.\")\n",
        "\n",
        "# Report the Best Hyperparameters and Scores\n",
        "print(\"\\n--- Gradient Boosting Classifier GridSearchCV Results ---\")\n",
        "print(f\"Best Hyperparameters: {grid_search_gb.best_params_}\")\n",
        "print(f\"Best Cross-Validated Recall: {grid_search_gb.best_score_:.4f}\")\n",
        "\n",
        "# Re-fit the Tuned Model and Evaluate on Test Set\n",
        "best_gb_tuned = grid_search_gb.best_estimator_\n",
        "\n",
        "# Make predictions on the test set with the tuned model\n",
        "y_test_pred_gb_tuned = best_gb_tuned.predict(X_test_scaled)\n",
        "tuned_gb_test_recall = recall_score(y_test, y_test_pred_gb_tuned)\n",
        "\n",
        "# Compare with Baseline\n",
        "# Assuming 'test_recall' is the baseline recall for the untuned Gradient Boosting model\n",
        "baseline_gb_recall = test_recall\n",
        "\n",
        "print(\"\\n--- Performance on Test Set ---\")\n",
        "print(f\"Baseline Test Recall (Untuned Gradient Boosting): {baseline_gb_recall:.4f}\")\n",
        "print(f\"Tuned Test Recall (GridSearch Gradient Boosting): {tuned_gb_test_recall:.4f}\")"
      ],
      "metadata": {
        "id": "FSAFzRNdXTyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7d912f0"
      },
      "source": [
        "Based on the Grid Search results focusing on maximizing recall, let's analyze the performance of the tuned Gradient Boosting, K-NN, and Random Forest models on the test set:\n",
        "\n",
        "*   **Tuned Gradient Boosting Classifier:** Achieved a Test Recall of **0.4534**. The Grid Search cross-validated recall was 0.7722.\n",
        "*   **Tuned K-Nearest Neighbors:** Achieved a Test Recall of **0.4273**. The Grid Search cross-validated recall was 0.7428.\n",
        "*   **Tuned Random Forest:** Achieved a Test Recall of **0.6486**. The Grid Search cross-validated recall was 0.8054.\n",
        "\n",
        "Comparing the Test Recall scores:\n",
        "\n",
        "The tuned Random Forest model achieved the highest test recall at **0.6486**, followed by the tuned Gradient Boosting Classifier at **0.4534**, and then the tuned K-Nearest Neighbors at **0.4273**.\n",
        "\n",
        "Therefore, based on the goal of prioritizing recall to catch failures, the **Tuned Random Forest** model is the best performing model among the three after applying Grid Search with the specified parameter grids."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Case of : XGB # We want to see if we can outperform the previous results\n",
        "import xgboost as xgb\n",
        "# Initialize and Train the XGBoost Classifier\n",
        "# scale_pos_weight is the XGBoost equivalent of class_weight='balanced'\n",
        "# It's calculated as: (count of negative class) / (count of positive class)\n",
        "# This tells the model to pay much more attention to the rare 'failure' class.\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic',\n",
        "                              eval_metric='logloss',\n",
        "                              scale_pos_weight=scale_pos_weight,\n",
        "                              random_state=42)\n",
        "# XGB doesn't take \"category\" inputs\n",
        "X_train_xgb= X_train_scaled.copy()\n",
        "X_test_xgb = X_test_scaled.copy()\n",
        "X_train_xgb['HPC outlet temperature (¬∞C)'] = X_train_scaled['HPC outlet temperature (¬∞C)'].astype(int)\n",
        "X_test_xgb['HPC outlet temperature (¬∞C)'] = X_test_scaled['HPC outlet temperature (¬∞C)'].astype(int)\n",
        "\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model.fit(X_train_xgb, y_train)\n",
        "\n",
        "#  Evaluate on the Test Set\n",
        "y_test_pred_xgb = xgb_model.predict(X_test_xgb)\n",
        "xgb_test_recall = recall_score(y_test, y_test_pred_xgb)\n",
        "\n",
        "print(\"\\n--- Model Performance Comparison ---\")\n",
        "print(f\"Tuned Random Forest Test Recall: {tuned_rf_test_recall:.4f}\")\n",
        "print(f\"Baseline XGBoost Test Recall: {xgb_test_recall:.4f}\")\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.1, 0.3],\n",
        "    'subsample': [0.8, 1.0] # Fraction of samples to be used for fitting each tree\n",
        "}\n",
        "\n",
        "# Set up and Run GridSearchCV\n",
        "\n",
        "# Initialize the base XGBoost model\n",
        "xgb_base = xgb.XGBClassifier(objective='binary:logistic',\n",
        "                             eval_metric='logloss',\n",
        "                             scale_pos_weight=scale_pos_weight,\n",
        "                             random_state=42)\n",
        "\n",
        "# Set up GridSearchCV to maximize recall\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb_base,\n",
        "                               param_grid=xgb_param_grid,\n",
        "                               scoring='recall',\n",
        "                               cv=3,\n",
        "                               n_jobs=-1,\n",
        "                               verbose=1) # Verbose shows progress\n",
        "\n",
        "print(\"Running GridSearchCV on XGBoost model...\")\n",
        "grid_search_xgb.fit(X_train_xgb, y_train)\n",
        "print(\"GridSearchCV finished.\")\n",
        "\n",
        "# Report the Best Hyperparameters and Scores\n",
        "print(\"\\n--- XGBoost GridSearchCV Results ---\")\n",
        "print(f\"Best Hyperparameters: {grid_search_xgb.best_params_}\")\n",
        "print(f\"Best Cross-Validated Recall: {grid_search_xgb.best_score_:.4f}\")\n",
        "\n",
        "best_xgb_tuned = grid_search_xgb.best_estimator_\n",
        "y_test_pred_xgb_tuned = best_xgb_tuned.predict(X_test_xgb)\n",
        "tuned_xgb_test_recall = recall_score(y_test, y_test_pred_xgb_tuned)\n",
        "\n",
        "# Compare with Baseline\n",
        "baseline_xgb_recall = 0.5119\n",
        "\n",
        "print(\"\\n--- Performance on Test Set ---\")\n",
        "print(f\"Baseline Test Recall (Untuned XGBoost): {baseline_xgb_recall:.4f}\")\n",
        "print(f\"Tuned Test Recall (GridSearch XGBoost): {tuned_xgb_test_recall:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-20T15:43:05.047591Z",
          "iopub.execute_input": "2025-08-20T15:43:05.047880Z",
          "iopub.status.idle": "2025-08-20T15:43:18.299286Z",
          "shell.execute_reply.started": "2025-08-20T15:43:05.047860Z",
          "shell.execute_reply": "2025-08-20T15:43:18.298433Z"
        },
        "id": "iFH9AFUyxd1q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "srNRwmmdaBgi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21bce817"
      },
      "source": [
        "Here is a summary of the Test Recall scores for all trained models:\n",
        "\n",
        "*   **Untuned Gradient Boosting:** 0.4577\n",
        "*   **Tuned Gradient Boosting:** 0.4534\n",
        "*   **Untuned K-NN:** 0.4273\n",
        "*   **Tuned K-NN:** 0.4273\n",
        "*   **Untuned Random Forest:** 0.3926\n",
        "*   **Tuned Random Forest:** **0.6486**\n",
        "*   **Untuned XGBoost:** 0.5119\n",
        "*   **Tuned XGBoost:** **0.7267**\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Prioritizing Recall to catch failures, the **Tuned XGBoost** model performed best on the test set with a Recall of **0.7267**. The **Tuned Random Forest** was the second best with a Recall of **0.6486**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Explainability with shap**\n",
        "\n",
        "The `shap` library (SHapley Additive exPlanations) is a Python library used for explaining the output of machine learning models. <br> It provides a unified framework for interpreting complex models and understanding the contributions of individual features to model predictions. <br>\n",
        "\n",
        "Shap is particularly useful for understanding black-box models like boosting, random forests, and deep neural networks, among others. <br>\n",
        "It can also be used with any classification model."
      ],
      "metadata": {
        "id": "Wbq6nxzYwFJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's install and import the shap library.**"
      ],
      "metadata": {
        "id": "m8XJ_7cpwFJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "SRYM5HygwFJa",
        "scrolled": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "np.bool=bool # code from last year"
      ],
      "metadata": {
        "id": "_oe1lXgYwFJa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shap is very heavy and takes a long time to compute. <br>\n",
        "To facilitate execution and reduce computing time, you can work on the **first 100 rows only**.\n",
        "\n",
        "*Note: You can use either the train features (X_train) or the test features (X_test) to compute shap values*"
      ],
      "metadata": {
        "id": "XZ1pnBXJwFJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_shap = X_train_xgb.iloc[:100]\n",
        "df_shap.info()"
      ],
      "metadata": {
        "id": "_TTiLTa4wyRT",
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 3.1**:</u> <br>\n",
        "**Create an object `explainer` that can compute shap values.** <br>\n",
        "\n",
        "*<u>Help</u>: You can use `shap.Explainer` for any trained classification model as input.* <br>\n",
        "*For tree based models, you can use `shap.TreeExplainer`*.\n"
      ],
      "metadata": {
        "id": "dDpCt-2owFJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SHAP explainer object for the best performing model (tuned XGBoost)\n",
        "# Use TreeExplainer for tree-based models like XGBoost\n",
        "explainer = shap.TreeExplainer(best_xgb_tuned)\n",
        "\n",
        "print(\"SHAP explainer object created.\")"
      ],
      "metadata": {
        "id": "zEbRO7Bk3Er6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, compute the shap values of a model with `explainer.shap_values`.** <br>\n",
        "If it takes too much time, you can reduce to 100-500 values."
      ],
      "metadata": {
        "id": "NXwDULXbwFJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute SHAP values for the first 100 rows of the scaled training data.\n",
        "# The explainer object was created for the tuned XGBoost model.\n",
        "shap_values = explainer.shap_values(df_shap)\n",
        "\n",
        "print(\"SHAP values computed.\")"
      ],
      "metadata": {
        "id": "dY54i16w3Er6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 3.2**:</u> <br>\n",
        "**Display the summary plot of shap values with `shap.summary_plot(...., plot_type=bar)`.**\n",
        "\n",
        "*Make sure you use `shap_values[0]` in your plot and not every shap value computed*"
      ],
      "metadata": {
        "id": "l6LpgXg2wFJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the summary plot as a bar chart\n",
        "# For binary classification, shap_values is already the matrix for the positive class.\n",
        "# Using shap_values[0] would incorrectly select a single row.\n",
        "shap.summary_plot(shap_values, df_shap, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "vvTGZlmrw2PC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 3.4**:</u> <br>\n",
        "**Use the same shap plot as previously but replace `plot_type=\"bar\"` with `plot_type=\"dot\"`.** <br>\n",
        "**And add the data you used to compute shap_values in `features=...`.**\n",
        "\n",
        "**Explain what you have understood about this plot**:\n",
        "- **Which variables are important in terms of explainability ?**\n",
        "- **How does the values of the important variables affect predictions ?**"
      ],
      "metadata": {
        "id": "cBzCZ8mEwFJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the summary plot as a dot plot\n",
        "# Using shap_values for binary classification is already the matrix for the positive class.\n",
        "shap.summary_plot(shap_values, df_shap, plot_type=\"dot\")"
      ],
      "metadata": {
        "id": "GYaRCaixw31F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Deep Learning**\n",
        "\n",
        "We will start by importing one of Python's Deep Learning libraries `tensorflow`/`keras`."
      ],
      "metadata": {
        "id": "T4QnxuN5wFJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout"
      ],
      "metadata": {
        "id": "lqUGsRQtwFJc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cells, `dataset` should be the dataset you transformed with data pre-processing (Onehotencoded, LabelEncoder,...). <br>\n",
        "You should run the following cells to prepare the data to train a Deep Learning model."
      ],
      "metadata": {
        "id": "VPm3ak4LwFJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>**Question 4.1**:</u> <br>\n",
        "**Make a small neural network model using `tensorflow`/`keras`, and print the accuracy**\n",
        "\n",
        "*Note: You can use the following elements to train the neural network* <br>\n",
        "- *`tf.keras.Sequential`*\n",
        "- *`layers.Dense(INTEGER, activation='relu')`*,\n",
        "- *`tf.keras.losses.BinaryCrossentropy`*\n",
        "- *`model.compile(optimizer='adam', ...)`*\n",
        "- *`model.fit`*\n",
        "- *`model.evaluate`* with epoch ~= 10"
      ],
      "metadata": {
        "id": "aY0cj9dfwFJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dropout(0.2), # to prevent overfitting\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    # Output layer for binary classification (sigmoid activation)\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nTraining the neural network model...\")\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2) # Use a validation split\n",
        "\n",
        "print(\"\\nNeural network model training finished.\")\n",
        "\n",
        "print(\"\\nEvaluating the model on the test set...\")\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "VkffQREV3Er6"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}